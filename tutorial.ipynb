{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Bot Demo with MLRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import create_and_set_project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_FILE = \"mlrun.env\" # update to your .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = create_and_set_project(env_file=ENV_FILE, git_source=\"git://github.com/mlrun/demo-llm-bot#main\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data and Deploy LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.run(\n",
    "    name=\"main\",\n",
    "    arguments={\n",
    "        \"source_directory\" : \"data/mlrun_docs_md\",\n",
    "        \"urls_file\" : \"data/urls/mlops_blogs.txt\"\n",
    "    },\n",
    "    watch=True,\n",
    "    dirty=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Required for inferencing depending on inside/outside container\n",
    "is_docker = os.path.exists('/.dockerenv') or os.path.isfile('/proc/self/cgroup') and any('docker' in line for line in open('/proc/self/cgroup'))\n",
    "host = \"host.docker.internal\" if is_docker else \"localhost\"\n",
    "\n",
    "# Get model serving endpoint port\n",
    "serving_fn = project.get_function(\"serve-llm\", sync=True)\n",
    "port = serving_fn.get_url().split(\":\")[-1]\n",
    "\n",
    "MODEL_ENDPOINT_URL = f\"http://{host}:{port}\"\n",
    "print(f\"Model endpoint: {MODEL_ENDPOINT_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(url=MODEL_ENDPOINT_URL, json={\"question\" : \"how I deploy ML models?\", \"chat_history\" : []})\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(url=MODEL_ENDPOINT_URL, json={\"question\" : \"How much do penguins weigh?\", \"chat_history\" : []})\n",
    "resp.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM via Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import messages_to_dict\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "def enrich_docs_url(ai_message: str) -> str:\n",
    "    pattern = r\"data/mlrun_docs_md/(.*?)\\.md\"\n",
    "    new_url = r\"https://docs.mlrun.org/en/latest/\\1.html\"\n",
    "    return re.sub(pattern, new_url, ai_message)\n",
    "\n",
    "\n",
    "def query_llm(message: str) -> str:\n",
    "    resp = requests.post(\n",
    "        url=MODEL_ENDPOINT_URL,\n",
    "        json={\n",
    "            \"question\": message,\n",
    "            \"chat_history\": messages_to_dict(memory.chat_memory.messages),\n",
    "        },\n",
    "        verify=False,\n",
    "    )\n",
    "    resp_json = resp.json()\n",
    "    ai_message = resp_json[\"output\"]\n",
    "    memory.save_context({\"input\": message}, {\"output\": ai_message})\n",
    "    ai_message = enrich_docs_url(ai_message=ai_message)\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def reset_memory() -> None:\n",
    "    memory.clear()\n",
    "    return None\n",
    "\n",
    "\n",
    "with gr.Blocks(analytics_enabled=False, theme=gr.themes.Soft()) as chat:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=6):\n",
    "            message = gr.Textbox(label=\"Q:\", placeholder=\"Type a question and Enter\")\n",
    "        with gr.Column(scale=3):\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = query_llm(message=message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    message.submit(respond, [message, chatbot], [message, chatbot])\n",
    "    clear.click(reset_memory, None, chatbot, queue=False)\n",
    "\n",
    "chat.launch(server_name=\"0.0.0.0\", share=True, ssl_verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
