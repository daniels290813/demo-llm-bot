{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive bot demo using LLMs and MLRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-06-16 09:51:44,066 [info] loaded project llamagpt from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlrun\n",
    "import openai\n",
    "\n",
    "# Copy mlrun.env and populate with relevant credentials\n",
    "ENV_FILE = \"us-cst-352-2.env\"\n",
    "\n",
    "mlrun.set_env_from_file(ENV_FILE)\n",
    "project = mlrun.get_or_create_project(\"llamagpt\")\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x29c541b20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from src import LlmIndex\n",
    "\n",
    "llmi = LlmIndex(additional_tools=[\"llm-math\"])\n",
    "\n",
    "llmi.add_index(\n",
    "    index_name=\"llama\",\n",
    "    documents=llmi.load_documents(documents_dir=\"data/sample\"),\n",
    "    summary=\"useful for questions about llamas\"\n",
    ")\n",
    "llmi.add_index(\n",
    "    index_name=\"strawberries\",\n",
    "    documents=llmi.load_documents(loader=\"WikipediaReader\", pages=['Strawberry']),\n",
    "    summary=\"useful for questions about strawberries\"\n",
    ")\n",
    "\n",
    "llmi.add_index(\n",
    "    index_name=\"mlops\",\n",
    "    documents=llmi.load_documents(\n",
    "        loader=\"BeautifulSoupWebReader\",\n",
    "        urls=['https://www.iguazio.com/blog/concept-drift-deep-dive-how-to-build-a-drift-aware-ml-system/']\n",
    "        ),\n",
    "    summary=\"useful for questions about mlops, machine learning, or concept drift\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llmi.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmi.persist(persist_dir=\"v3io:///bigdata/storage\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-16 09:52:06,047 [info] Loading indices with ids: ['llama']\n",
      "Loaded llama index useful for questions about llamas\n",
      "2023-06-16 09:52:06,048 [info] Loading indices with ids: ['strawberries']\n",
      "Loaded strawberries index useful for questions about strawberries\n",
      "2023-06-16 09:52:06,049 [info] Loading indices with ids: ['mlops']\n",
      "Loaded mlops index useful for questions about mlops, machine learning, or concept drift\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llama': 'useful for questions about llamas',\n",
       " 'strawberries': 'useful for questions about strawberries',\n",
       " 'mlops': 'useful for questions about mlops, machine learning, or concept drift'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the docs & indexes from storage and run queries\n",
    "llmi = LlmIndex(additional_tools=[\"llm-math\"])\n",
    "llmi.load_storage()\n",
    "llmi.get_catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-16 09:52:07,294 [info] Loading indices with ids: ['llama']\n",
      "2023-06-16 09:52:07,380 [info] Loading indices with ids: ['strawberries']\n",
      "2023-06-16 09:52:07,381 [info] Loading indices with ids: ['mlops']\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: llama\n",
      "Action Input: How tall are llamas?\u001b[0m2023-06-16 09:52:08,042 [info] > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-06-16 09:52:08,043 [info] > [retrieve] Total embedding token usage: 6 tokens\n",
      "2023-06-16 09:52:09,200 [info] > [get_response] Total LLM token usage: 1835 tokens\n",
      "2023-06-16 09:52:09,200 [info] > [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "Llamas are typically between 5 and 6 feet tall.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: Llamas are typically between 5 and 6 feet tall.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Llamas are typically between 5 and 6 feet tall.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: strawberries\n",
      "Action Input: countries that produce strawberries\u001b[0m2023-06-16 09:52:10,639 [info] > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-06-16 09:52:10,640 [info] > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-06-16 09:52:11,540 [info] > [get_response] Total LLM token usage: 1750 tokens\n",
      "2023-06-16 09:52:11,541 [info] > [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\n",
      "China, United States, Turkey, France, Chile, England, Poland\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: Some of the top countries that produce strawberries include China, the United States, Turkey, France, Chile, England, and Poland. However, many other countries also produce strawberries on a smaller scale.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Some of the top countries that produce strawberries include China, the United States, Turkey, France, Chile, England, and Poland. However, many other countries also produce strawberries on a smaller scale.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: mlops\n",
      "Action Input: What is MLOps?\u001b[0m2023-06-16 09:52:13,840 [info] > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-06-16 09:52:13,841 [info] > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-06-16 09:52:15,778 [info] > [get_response] Total LLM token usage: 144 tokens\n",
      "2023-06-16 09:52:15,780 [info] > [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m\n",
      "MLOps (also known as DevOps for Machine Learning) is a practice that combines software development and operations processes to enable the rapid and reliable delivery of machine learning models. It is designed to help organizations automate and streamline the process of building, testing, and deploying machine learning models.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: MLOps is an important practice for organizations that are working with machine learning models. By combining software development and operations processes, MLOps can help to ensure that machine learning models are delivered quickly and reliably. This can be especially important in industries such as finance, healthcare, and manufacturing, where machine learning models are increasingly being used to make important decisions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MLOps is an important practice for organizations that are working with machine learning models. By combining software development and operations processes, MLOps can help to ensure that machine learning models are delivered quickly and reliably. This can be especially important in industries such as finance, healthcare, and manufacturing, where machine learning models are increasingly being used to make important decisions.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: 5 * 250 pounds\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 1250\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: The average weight of a llama is around 250 pounds, so 5 times that would be 1250 pounds.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The average weight of a llama is around 250 pounds, so 5 times that would be 1250 pounds.\n"
     ]
    }
   ],
   "source": [
    "print(llmi.query(\"How tall are llamas?\"))\n",
    "print(llmi.query(\"Which countries produce strawberries?\"))\n",
    "print(llmi.query(\"What is MLOps?\"))\n",
    "print(llmi.query(\"What 5 times the average weight of a llama?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-06-16 09:52:24,478 [info] Deploying serving function MOCK (for simulation)...\n",
      "Turn off the mock (mock=False) and make sure Nuclio is installed for real deployment to Nuclio\n",
      "2023-06-16 09:52:25,859 [info] Loading indices with ids: ['llama']\n",
      "Loaded llama index useful for questions about llamas\n",
      "2023-06-16 09:52:25,861 [info] Loading indices with ids: ['strawberries']\n",
      "Loaded strawberries index useful for questions about strawberries\n",
      "2023-06-16 09:52:25,863 [info] Loading indices with ids: ['mlops']\n",
      "Loaded mlops index useful for questions about mlops, machine learning, or concept drift\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeployStatus(state=ready, outputs={'endpoint': 'Mock', 'name': 'llama'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = mlrun.code_to_function(\n",
    "    name=\"llama\",\n",
    "    filename=\"src/llmindex.py\",\n",
    "    kind=\"serving\",\n",
    "    image=\"mlrun/mlrun:1.3.2\",\n",
    "    requirements=[\"llama-index==0.6.26\", \"langchain==0.0.200\"]\n",
    ")\n",
    "graph = fn.set_topology(\"flow\", engine=\"async\")\n",
    "graph.add_step(name=\"llama\", class_name=\"QueryLlamaGpt\", persist_dir=\"v3io:///bigdata/storage\", additional_tools=[\"llm-math\"], full_event=True).respond()\n",
    "fn.set_env(\"OPENAI_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "fn.set_env(\"OPENAI_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "\n",
    "project.deploy_function(fn, mock=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How tall are llamas?\"\n",
    "resp = fn.invoke(path=\"query\", body={\"query\" : query})\n",
    "resp[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 50 times the height of the tallest llama in the world?\"\n",
    "resp = fn.invoke(path=\"query\", body={\"query\" : query})\n",
    "resp[\"response\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected 3 arguments for function <function respond at 0x2aeaf8f70>, received 2.\n",
      "Expected at least 3 arguments for function <function respond at 0x2aeaf8f70>, received 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-16 09:52:41,682 [info] Loading indices with ids: ['llama']\n",
      "2023-06-16 09:52:41,683 [info] Loading indices with ids: ['strawberries']\n",
      "2023-06-16 09:52:41,684 [info] Loading indices with ids: ['mlops']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument. Filling with None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAI: I'm just a machine learning model, so I don't have feelings in the way humans do, but I'm functioning properly and ready to assist you with any questions or tasks you have. How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: llama\n",
      "Action Input: tell me about llamas\u001b[0m2023-06-16 09:52:52,593 [info] > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-06-16 09:52:52,594 [info] > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-06-16 09:53:00,523 [info] > [get_response] Total LLM token usage: 2111 tokens\n",
      "2023-06-16 09:53:00,524 [info] > [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "Llamas are domesticated South American camelids that have been used as a meat and pack animal by Andean cultures since the Pre-Columbian era. They are social animals and live in herds. Their wool is soft and contains only a small amount of lanolin. They can learn simple tasks after a few repetitions and can carry about 25 to 30% of their body weight for 8 to 13 km (5–8 miles). In Aymara mythology, llamas are important beings. \n",
      "\n",
      "Llamas are classified as Lamoids, which consist of the vicuña, guanaco, Suri alpaca, and Huacaya alpaca, and the domestic llama. They are the sole existing representatives of a very distinct section of the Artiodactyla or even-toed ungulates, called Tylopoda. \n",
      "\n",
      "When correctly reared, llamas are very friendly and pleasant to be around. They are extremely curious and most will approach people easily. Llamas have started showing up in nursing homes and hospitals as certified therapy animals. They make groaning noises or going \"mwa\" (/mwaʰ/) when they are fearful or angry. \n",
      "\n",
      "Llamas are also used as guard animals to protect livestock from predators. Typically, a single gelded male of two years of age is used and bonds quickly with its new charges. Using llamas as guards has reduced the losses to predators for many producers.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: Llamas are fascinating animals that have been domesticated for thousands of years. They are social animals that live in herds and are known for their soft wool and ability to carry heavy loads. They are also used as therapy animals and as guards to protect livestock from predators.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: llama\n",
      "Action Input: how many llamas are in the United States?\u001b[0m2023-06-16 09:53:18,224 [info] > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-06-16 09:53:18,224 [info] > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-06-16 09:53:20,062 [info] > [get_response] Total LLM token usage: 1890 tokens\n",
      "2023-06-16 09:53:20,063 [info] > [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "As of 2007, there were over 158,000 llamas in the United States.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: Llamas are becoming increasingly popular in the United States as pets, pack animals, and for their wool. They are also used in therapy programs and as show animals.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: llama\n",
      "Action Input: how many llamas are there in the world?\u001b[0m2023-06-16 09:53:30,637 [info] > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-06-16 09:53:30,638 [info] > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-06-16 09:53:34,446 [info] > [get_response] Total LLM token usage: 1933 tokens\n",
      "2023-06-16 09:53:34,446 [info] > [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "As of 2007, there were over seven million llamas and alpacas in South America and over 158,000 llamas and 100,000 alpacas, descended from progenitors imported late in the 20th century, in the United States and Canada. Therefore, the total number of llamas in the world is estimated to be around 8.158 million.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: The total number of llamas in the world is estimated to be around 8.158 million, with the majority of them being in South America. Llamas are fascinating animals with a long history of domestication and use by humans for a variety of purposes. They are known for their soft wool, ability to carry heavy loads, and social nature.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: 8.158 million * 50\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 407900000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: The answer to your question \"what is 50 times the total number of llamas in the world?\" is 407,900,000.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def query_llama(query: str) -> str:\n",
    "    resp = fn.invoke(path=\"/query\", body={\"query\" : query})\n",
    "    query_result = resp[\"response\"]\n",
    "    return query_result\n",
    "\n",
    "with gr.Blocks(analytics_enabled=False, theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=6):\n",
    "            msg = gr.Textbox(label=\"Q:\", placeholder=\"Type a question and Enter\")\n",
    "        with gr.Column(scale=3):\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, chat_history, index):\n",
    "        bot_message = query_llama(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
